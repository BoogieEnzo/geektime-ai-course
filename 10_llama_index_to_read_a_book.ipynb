{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install llama-index\n",
    "%pip install langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 先搜索，后提示"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 6770 tokens\n"
     ]
    }
   ],
   "source": [
    "import openai, os\n",
    "from llama_index import GPTSimpleVectorIndex, SimpleDirectoryReader\n",
    "\n",
    "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "documents = SimpleDirectoryReader('./data/mr_fujino').load_data()\n",
    "index = GPTSimpleVectorIndex.from_documents(documents)\n",
    "\n",
    "index.save_to_disk('index_mr_fujino.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 2991 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 34 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "鲁迅先生在日本学习医学的老师是藤野严九郎先生。\n"
     ]
    }
   ],
   "source": [
    "index = GPTSimpleVectorIndex.load_from_disk('index_mr_fujino.json')\n",
    "response = index.query(\"鲁迅先生在日本学习医学的老师是谁？\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36;1m\u001b[1;3m> Got node text: 藤野先生\n",
      "\n",
      "鲁迅\n",
      "\n",
      "东京也无非是这样。上野的樱花烂熳的时节，望去确也像绯红的轻云，但花下也缺不了成群结队的“清国留学生”的速成班，头顶上盘着大辫子，顶得学生制帽的顶上高高耸起，形成一座富士山。也有解散辫子，盘得平的，除下帽来，油光可鉴，宛如小姑娘的发髻一般，还要将脖子扭几扭。实在标致极了。\n",
      "\n",
      "中国留学生会馆的门房里有几本书买，有时还值得去一转；倘在上午，里面的几间洋房里倒也还可以坐坐的。但到...\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 2975 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 25 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "鲁迅先生去仙台的医学专门学校学习医学。\n"
     ]
    }
   ],
   "source": [
    "response = index.query(\"鲁迅先生去哪里学的医学？\", verbose=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36;1m\u001b[1;3m> Got node text: 藤野先生\n",
      "\n",
      "鲁迅\n",
      "\n",
      "东京也无非是这样。上野的樱花烂熳的时节，望去确也像绯红的轻云，但花下也缺不了成群结队的“清国留学生”的速成班，头顶上盘着大辫子，顶得学生制帽的顶上高高耸起，形成一座富士山。也有解散辫子，盘得平的，除下帽来，油光可鉴，宛如小姑娘的发髻一般，还要将脖子扭几扭。实在标致极了。\n",
      "\n",
      "中国留学生会馆的门房里有几本书买，有时还值得去一转；倘在上午，里面的几间洋房里倒也还可以坐坐的。但到...\n",
      "\u001b[0m"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 3014 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 25 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "不知道\n"
     ]
    }
   ],
   "source": [
    "from llama_index import QuestionAnswerPrompt\n",
    "query_str = \"鲁迅先生去哪里学的医学？\"\n",
    "QA_PROMPT_TMPL = (\n",
    "    \"下面的“我”指的是鲁迅先生 \\n\"\n",
    "    \"---------------------\\n\"\n",
    "    \"{context_str}\"\n",
    "    \"\\n---------------------\\n\"\n",
    "    \"根据这些信息，请回答问题: {query_str}\\n\"\n",
    "    \"如果您不知道的话，请回答不知道\\n\"\n",
    ")\n",
    "QA_PROMPT = QuestionAnswerPrompt(QA_PROMPT_TMPL)\n",
    "# Build GPTSimpleVectorIndex\n",
    "\n",
    "response = index.query(query_str, text_qa_template=QA_PROMPT, verbose=True)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 3027 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 37 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "不知道\n"
     ]
    }
   ],
   "source": [
    "response = index.query(\"请问林黛玉和贾宝玉是什么关系？\", text_qa_template=QA_PROMPT)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 通过llama_index对于文章进行小结"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install spacy\n",
    "%python -m spacy download zh_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:llama_index.llm_predictor.base:Unknown max input size for gpt-3.5-turbo, using defaults.\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 0 tokens\n",
      "INFO:llama_index.indices.common_tree.base:> Building index from nodes: 2 chunks\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 9787 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "鲁迅先生回忆了自己在日本学医期间的经历，描述了自己在解剖实习中的经历，以及与教授藤野先生的交往。他还提到了一些不愉快的事情，比如遭到同学的诽谤和歧视，以及看到中国人被枪毙时的感受。最后，他告诉藤野先生自己将不再学医，而是想学生物学。他想起了一个人，这个人是他的老师，他对鲁迅很热心，给他很多鼓励和教诲。鲁迅现在只有他的照片，但是每次看到他的照片，都会让他感到勇气和良心发现。\n"
     ]
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.text_splitter import SpacyTextSplitter\n",
    "from llama_index import GPTListIndex, LLMPredictor, ServiceContext\n",
    "from llama_index.node_parser import SimpleNodeParser\n",
    "\n",
    "# define LLM\n",
    "llm_predictor = LLMPredictor(llm=ChatOpenAI(temperature=0, model_name=\"gpt-3.5-turbo\", max_tokens=1024))\n",
    "\n",
    "text_splitter = SpacyTextSplitter(pipeline=\"zh_core_web_sm\", chunk_size = 2048)\n",
    "parser = SimpleNodeParser(text_splitter=text_splitter)\n",
    "documents = SimpleDirectoryReader('./data/mr_fujino').load_data()\n",
    "nodes = parser.get_nodes_from_documents(documents)\n",
    "\n",
    "service_context = ServiceContext.from_defaults(llm_predictor=llm_predictor)\n",
    "\n",
    "list_index = GPTListIndex(nodes=nodes, service_context=service_context)\n",
    "response = list_index.query(\"下面鲁迅先生以第一人称‘我’写的内容，请你用中文总结一下:\", response_mode=\"tree_summarize\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total LLM token usage: 0 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [build_index_from_nodes] Total embedding token usage: 0 tokens\n",
      "INFO:llama_index.indices.query.tree.summarize_query:> Starting query: 下面鲁迅先生以第一人称‘我’写的内容，请你用中文总结一下:\n",
      "INFO:llama_index.indices.common_tree.base:> Building index from nodes: 2 chunks\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total LLM token usage: 9653 tokens\n",
      "INFO:llama_index.token_counter.token_counter:> [query] Total embedding token usage: 0 tokens\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "鲁迅先生回忆了自己在日本学医的经历，描述了自己在解剖实习中遇到的困难和藤野先生的态度，以及自己在学校中遭受的歧视和流言。最后，他告诉藤野先生自己不再学医，离开仙台，并且与藤野先生告别。他想写信给一个老师，但不知道该写些什么。这个老师的名字不为人所知，但他的性格很伟大。鲁迅现在只有他的照片，但每次看到照片都会让他感到勇气和良心发现。\n"
     ]
    }
   ],
   "source": [
    "from llama_index import GPTTreeIndex\n",
    "\n",
    "# define LLM\n",
    "tree_index = GPTTreeIndex(nodes=nodes, service_context=service_context)\n",
    "response = tree_index.query(\"下面鲁迅先生以第一人称‘我’写的内容，请你用中文总结一下:\", mode=\"summarize\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 支持图片的多模态"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration.\n"
     ]
    }
   ],
   "source": [
    "from llama_index import SimpleDirectoryReader, GPTSimpleVectorIndex\n",
    "from llama_index.readers.file.base import DEFAULT_FILE_EXTRACTOR, ImageParser\n",
    "from llama_index.response.notebook_utils import display_response, display_image\n",
    "from llama_index.indices.query.query_transform.base import ImageOutputQueryTransform\n",
    "\n",
    "image_parser = ImageParser(keep_image=True, parse_text=True)\n",
    "file_extractor = DEFAULT_FILE_EXTRACTOR\n",
    "file_extractor.update(\n",
    "{\n",
    "    \".jpg\": image_parser,\n",
    "    \".png\": image_parser,\n",
    "    \".jpeg\": image_parser,\n",
    "})\n",
    "\n",
    "# NOTE: we add filename as metadata for all documents\n",
    "filename_fn = lambda filename: {'file_name': filename}\n",
    "\n",
    "receipt_reader = SimpleDirectoryReader(\n",
    "    input_dir='./data/receipts', \n",
    "    file_extractor=file_extractor, \n",
    "    file_metadata=filename_fn,\n",
    ")\n",
    "receipt_documents = receipt_reader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The constructor now takes in a list of Node objects. Since you are passing in a list of Document objects, please use `from_documents` instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m receipts_index \u001b[38;5;241m=\u001b[39m \u001b[43mGPTSimpleVectorIndex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreceipt_documents\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/geektime/lib/python3.10/site-packages/llama_index/indices/vector_store/vector_indices.py:94\u001b[0m, in \u001b[0;36mGPTSimpleVectorIndex.__init__\u001b[0;34m(self, nodes, index_struct, service_context, text_qa_template, simple_vector_store_data_dict, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m     simple_vector_store_data_dict \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     87\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124membedding_dict\u001b[39m\u001b[38;5;124m\"\u001b[39m: index_struct\u001b[38;5;241m.\u001b[39membeddings_dict,\n\u001b[1;32m     88\u001b[0m     }\n\u001b[1;32m     90\u001b[0m vector_store \u001b[38;5;241m=\u001b[39m SimpleVectorStore(\n\u001b[1;32m     91\u001b[0m     simple_vector_store_data_dict\u001b[38;5;241m=\u001b[39msimple_vector_store_data_dict\n\u001b[1;32m     92\u001b[0m )\n\u001b[0;32m---> 94\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     95\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_struct\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_struct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m    \u001b[49m\u001b[43mservice_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mservice_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtext_qa_template\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtext_qa_template\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvector_store\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvector_store\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;66;03m# TODO: Temporary hack to also store embeddings in index_struct\u001b[39;00m\n\u001b[1;32m    104\u001b[0m embedding_dict \u001b[38;5;241m=\u001b[39m vector_store\u001b[38;5;241m.\u001b[39m_data\u001b[38;5;241m.\u001b[39membedding_dict\n",
      "File \u001b[0;32m~/miniconda3/envs/geektime/lib/python3.10/site-packages/llama_index/indices/vector_store/base.py:58\u001b[0m, in \u001b[0;36mGPTVectorStoreIndex.__init__\u001b[0;34m(self, nodes, index_struct, service_context, text_qa_template, vector_store, use_async, **kwargs)\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtext_qa_template \u001b[38;5;241m=\u001b[39m text_qa_template \u001b[38;5;129;01mor\u001b[39;00m DEFAULT_TEXT_QA_PROMPT\n\u001b[1;32m     57\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_use_async \u001b[38;5;241m=\u001b[39m use_async\n\u001b[0;32m---> 58\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m     59\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     60\u001b[0m \u001b[43m    \u001b[49m\u001b[43mindex_struct\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_struct\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     61\u001b[0m \u001b[43m    \u001b[49m\u001b[43mservice_context\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mservice_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     62\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/geektime/lib/python3.10/site-packages/llama_index/indices/base.py:56\u001b[0m, in \u001b[0;36mBaseGPTIndex.__init__\u001b[0;34m(self, nodes, index_struct, docstore, service_context)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nodes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(nodes) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(nodes[\u001b[38;5;241m0\u001b[39m], Node):\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(nodes[\u001b[38;5;241m0\u001b[39m], Document):\n\u001b[0;32m---> 56\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m     57\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe constructor now takes in a list of Node objects. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     58\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSince you are passing in a list of Document objects, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     59\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mplease use `from_documents` instead.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     60\u001b[0m         )\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnodes must be a list of Node objects.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: The constructor now takes in a list of Node objects. Since you are passing in a list of Document objects, please use `from_documents` instead."
     ]
    }
   ],
   "source": [
    "receipts_index = GPTSimpleVectorIndex.from_documents(receipt_documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "receipts_response = receipts_index.query(\n",
    "    'When was the last time I went to McDonald\\'s and how much did I spend. \\\n",
    "    Also show me the receipt from my visit.',\n",
    "    query_transform=ImageOutputQueryTransform(width=400)\n",
    ")\n",
    "\n",
    "display_response(receipts_response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "8114e84f04cf14e493992e1b725447accf84073d5ec18e7063d492738bf032cb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
